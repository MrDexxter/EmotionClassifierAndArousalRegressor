{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiU644xA-ORP",
        "outputId": "a887b9cc-acb0-4ae9-bbb2-8176c4d10ba0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7qH4UQr-TPU"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "my_tar = tarfile.open(\"drive/MyDrive/train_and_val_set.tar\")\n",
        "my_tar.extractall(\"train_and_val_set\")\n",
        "my_tar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MYsO_LU_A5E"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import StepLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6NGf6Y9WCwn"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRoov0CkBDXJ"
      },
      "outputs": [],
      "source": [
        "class AffectDataset(Dataset):\n",
        "    def __init__(self, train_path, transform=None):\n",
        "        self.images_path = os.path.join(train_path, 'images')\n",
        "        self.annotations_path = os.path.join(train_path, 'annotations')\n",
        "        self.transform = transform\n",
        "\n",
        "        self.image_files = sorted(os.listdir(self.images_path))\n",
        "        self.annotation_files = sorted(os.listdir(self.annotations_path))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # print(self.images_path)\n",
        "        image = Image.open(os.path.join(self.images_path, self.image_files[idx])).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        annotation_path = os.path.join(self.annotations_path, self.image_files[idx] ).replace(\".jpg\", \"_exp.npy\")\n",
        "  \n",
        "        landmarks = np.load(annotation_path.replace(\"_exp.npy\", \"_lnd.npy\"))\n",
        "   \n",
        "        landmarks = torch.tensor(landmarks, dtype=torch.float32)\n",
        "        \n",
        "        arousal = float(np.load(annotation_path.replace(\"_exp.npy\", \"_aro.npy\")))\n",
        "        valence = float(np.load(annotation_path.replace(\"_exp.npy\", \"_val.npy\")))\n",
        "        \n",
        "        \n",
        "        arousal_valence = torch.tensor([arousal, valence], dtype=torch.float32)\n",
        "\n",
        "        expression = float(np.load(annotation_path))\n",
        "        expression = torch.tensor(expression, dtype=torch.long)\n",
        "        \n",
        "        return image, landmarks, arousal_valence, expression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbZjRMfVBWlk"
      },
      "outputs": [],
      "source": [
        "class MultiTaskModel(nn.Module):\n",
        "    def __init__(self, model_name='resnet18'):\n",
        "        super(MultiTaskModel, self).__init__()\n",
        "        if model_name == 'resnet18':\n",
        "            self.resnet = models.resnet18(pretrained=True)\n",
        "            num_ftrs = self.resnet.fc.in_features\n",
        "            self.resnet.fc = nn.Linear(num_ftrs, 512)\n",
        "            for param in self.resnet.parameters():\n",
        "                param.requires_grad = False\n",
        "        elif model_name == 'densenet121':\n",
        "            self.resnet = models.densenet121(pretrained=True)\n",
        "            num_ftrs = self.resnet.classifier.in_features\n",
        "            self.resnet.classifier = nn.Linear(num_ftrs, 512)\n",
        "            for param in self.resnet.parameters():\n",
        "                param.requires_grad = False\n",
        "        else:\n",
        "            raise ValueError(\"Invalid model_name\")\n",
        "\n",
        "        self.landmark_layer = nn.Sequential(\n",
        "            nn.Linear(136, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.concat_layer = nn.Sequential(\n",
        "            nn.Linear(512 + 128, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.arousal_valence_head = nn.Linear(256, 2)\n",
        "        self.expression_head = nn.Linear(256, 8)\n",
        "\n",
        "    def forward(self, x, landmarks):\n",
        "        landmarks.to(device)\n",
        "        x = self.resnet(x)\n",
        "        landmarks = self.landmark_layer(landmarks)\n",
        "        \n",
        "        x = torch.cat((x, landmarks), dim=1)\n",
        "       \n",
        "        x = self.concat_layer(x)\n",
        "      \n",
        "        arousal_valence = self.arousal_valence_head(x)\n",
        "        expression = self.expression_head(x)\n",
        "        return arousal_valence, expression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_FSUVBiBYyo"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(loader, model, device):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, landmarks, arousal_valence_labels, expression_labels in loader:\n",
        "            images = images.to(device)\n",
        "            landmarks = landmarks.to(device)  # Add this line\n",
        "            expression_labels = expression_labels.to(device)\n",
        "\n",
        "            _, expression_preds = model(images, landmarks)  # Pass the landmarks tensor here\n",
        "            _, predicted = torch.max(expression_preds.data, 1)\n",
        "            total += expression_labels.size(0)\n",
        "            correct += (predicted == expression_labels).sum().item()\n",
        "\n",
        "    return 100 * correct / total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2cPWjDsBb9i"
      },
      "outputs": [],
      "source": [
        "# Dataset preparation and pre-processing\n",
        "# Add data augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomGrayscale(p=0.1),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "dataset = AffectDataset(\"train_and_val_set/train_set/\", transform_train)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Bv4DBksBfgj",
        "outputId": "77efb67c-f515-4c31-deca-606a033696b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|██████████| 30.8M/30.8M [00:00<00:00, 154MB/s]\n"
          ]
        }
      ],
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MultiTaskModel(model_name=\"densenet121\").to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "criterion1 = nn.MSELoss()\n",
        "criterion2 = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "sRxhn2eOBhu5",
        "outputId": "dacf0d4b-9285-4f88-eb12-bc50a7996a32"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [1/1]:   0%|          | 1/7192 [00:36<72:10:10, 36.13s/it, loss=2.55]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-37ef92c46075>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0marousal_valence_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpression_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mloss1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marousal_valence_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marousal_valence_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpression_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpression_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-713a9a7efe99>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mlandmarks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/densenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madaptive_avg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/densenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, init_features)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minit_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mnew_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/densenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mbottleneck_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_checkpoint_bottleneck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mbottleneck_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mnew_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottleneck_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/densenet.py\u001b[0m in \u001b[0;36mbn_function\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbn_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mconcated_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mbottleneck_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcated_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: T484\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbottleneck_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=True)\n",
        "    for i, (images, landmarks, arousal_valence_labels, expression_labels) in loop:\n",
        "        \n",
        "        images = images.to(device)\n",
        "        landmarks = landmarks.to(device)\n",
        "        arousal_valence_labels = arousal_valence_labels.to(device)\n",
        "        expression_labels = expression_labels.to(device)\n",
        "\n",
        "        # rest of the code\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        arousal_valence_preds, expression_preds = model(images, landmarks)\n",
        "        loss1 = criterion1(arousal_valence_preds, arousal_valence_labels)\n",
        "        loss2 = criterion2(expression_preds, expression_labels)\n",
        "        loss = loss1 + loss2\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        loop.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "        loop.set_postfix(loss=running_loss / (i + 1))\n",
        "\n",
        "    train_losses.append(running_loss / len(train_loader))\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    val_loop = tqdm(enumerate(val_loader), total=len(val_loader), leave=True)\n",
        "    with torch.no_grad():\n",
        "        for i, (images, landmarks, arousal_valence_labels, expression_labels) in val_loop:\n",
        "            images = images.to(device)\n",
        "            landmarks = landmarks.to(device)\n",
        "            arousal_valence_labels = arousal_valence_labels.to(device)\n",
        "            expression_labels = expression_labels.to(device)\n",
        "\n",
        "            arousal_valence_preds, expression_preds = model(images, landmarks)\n",
        "            val_loss1 = criterion1(arousal_valence_preds, arousal_valence_labels)\n",
        "            val_loss2 = criterion2(expression_preds, expression_labels)\n",
        "            val_loss = val_loss1 + val_loss2\n",
        "\n",
        "            running_val_loss += val_loss.item()\n",
        "            val_loop.set_description(f\"Validation Epoch [{epoch+1}/{num_epochs}]\")\n",
        "            val_loop.set_postfix(val_loss=running_val_loss / (i + 1))\n",
        "\n",
        "    val_losses.append(running_val_loss / len(val_loader))\n",
        "\n",
        "    # Update the learning rate using the scheduler\n",
        "    scheduler.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "Mca0cHewBkAN",
        "outputId": "fc3c5dc7-a249-4e97-a5d3-bbfb5189ac6f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_losses, label='ResNet Loss', linestyle='-', marker='o', color='blue')\n",
        "plt.plot(val_losses, label='ResNet Validation Loss', linestyle='-', marker='o', color='red')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eBilVdqPTcf"
      },
      "source": [
        "## Testing the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYOUTVq8s3wm"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "my_tar = tarfile.open(\"drive/MyDrive/test_set.tar\")\n",
        "my_tar.extractall(\"test_set\")\n",
        "my_tar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w92R0JDhs9WP"
      },
      "outputs": [],
      "source": [
        "# Dataset preparation and pre-processing\n",
        "# Add data augmentation\n",
        "test_dataset = AffectDataset(\"test_set/val_set\", transform_train)\n",
        "test_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIGYbj-2ud5r",
        "outputId": "7c11d55b-3ef4-443e-dec4-3fe5b4f43fdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted_traits:\n",
            "emotion =  Neutral\n",
            "Arousal =  0.14186443\n",
            "Valence =  -0.22131512\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def predict_image(image_path, landmarks_path, model, device, transform):\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Landmarks are needed to make a prediction\n",
        "    # Replace this with actual landmarks from your dataset or acquired by other means\n",
        "    landmarks = np.load(landmarks_path)\n",
        "   \n",
        "    landmarks = torch.tensor(landmarks, dtype=torch.float32).to(device)\n",
        "    landmarks = landmarks.reshape(1, -1)\n",
        "    # Make the prediction\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        arousal_valence_preds, expression_preds = model(image_tensor, landmarks)\n",
        "\n",
        "    arousal_valence = arousal_valence_preds.cpu().numpy()\n",
        "    expression = torch.argmax(expression_preds, dim=1).cpu().numpy()\n",
        "\n",
        "    return arousal_valence, expression\n",
        "\n",
        "# Usage example\n",
        "image_path = \"test_set/val_set/images/184.jpg\"\n",
        "landmarks_path = \"test_set/val_set/annotations/184_lnd.npy\"\n",
        "# model = model.to(device)\n",
        "arousal_valence, emotions = predict_image(image_path, landmarks_path, model, device, transform_train)\n",
        "emotion_dict = {0: \"Neutral\", 1: \"Happy\", 2: \"Sad\", 3: \"Surprise\", 4:\n",
        "\"Fear\", 5: \"Disgust\", 6: \"Anger\", 7: 'Contempt'}\n",
        "\n",
        "print(\"Predicted_traits:\")\n",
        "\n",
        "print(\"emotion = \", emotion_dict[emotions.item()])\n",
        "print(\"Arousal = \", arousal_valence[0][0])\n",
        "print(\"Valence = \", arousal_valence[0][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xg-liSk6vFb-"
      },
      "outputs": [],
      "source": [
        "model_path = \"resnet_back.pth\"\n",
        "torch.save(model.state_dict(), model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu6sGiL4PZl4"
      },
      "source": [
        "## Training the other model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlHy4UAxzQe3",
        "outputId": "08d0a949-184e-4b67-9325-eaf73f6b1449"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|██████████| 30.8M/30.8M [00:00<00:00, 85.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MultiTaskModel(model_name=\"densenet121\").to(device)\n",
        "\n",
        "# Fine-tuning the model\n",
        "# for param in model.resnet.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# num_ftrs = model.resnet.fc.in_features\n",
        "# model.resnet.fc = nn.Linear(num_ftrs, 512)\n",
        "\n",
        "# Define the optimizer and learning rate scheduler\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "criterion1 = nn.MSELoss()\n",
        "criterion2 = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgZFva4mzy4T",
        "outputId": "a43f3666-c494-4a5b-dbe7-038035decdd6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [1/1]: 100%|██████████| 7192/7192 [1:12:44<00:00,  1.65it/s, loss=0.917]\n",
            "Validation Epoch [1/1]: 100%|██████████| 1798/1798 [11:18<00:00,  2.65it/s, val_loss=0.771]\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=True)\n",
        "    for i, (images, landmarks, arousal_valence_labels, expression_labels) in loop:\n",
        "        \n",
        "        images = images.to(device)\n",
        "        landmarks = landmarks.to(device)\n",
        "        arousal_valence_labels = arousal_valence_labels.to(device)\n",
        "        expression_labels = expression_labels.to(device)\n",
        "\n",
        "        # rest of the code\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        arousal_valence_preds, expression_preds = model(images, landmarks)\n",
        "        loss1 = criterion1(arousal_valence_preds, arousal_valence_labels)\n",
        "        loss2 = criterion2(expression_preds, expression_labels)\n",
        "        loss = loss1 + loss2\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        loop.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "        loop.set_postfix(loss=running_loss / (i + 1))\n",
        "\n",
        "    train_losses.append(running_loss / len(train_loader))\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    val_loop = tqdm(enumerate(val_loader), total=len(val_loader), leave=True)\n",
        "    with torch.no_grad():\n",
        "        for i, (images, landmarks, arousal_valence_labels, expression_labels) in val_loop:\n",
        "            images = images.to(device)\n",
        "            landmarks = landmarks.to(device)\n",
        "            arousal_valence_labels = arousal_valence_labels.to(device)\n",
        "            expression_labels = expression_labels.to(device)\n",
        "\n",
        "            arousal_valence_preds, expression_preds = model(images, landmarks)\n",
        "            val_loss1 = criterion1(arousal_valence_preds, arousal_valence_labels)\n",
        "            val_loss2 = criterion2(expression_preds, expression_labels)\n",
        "            val_loss = val_loss1 + val_loss2\n",
        "\n",
        "            running_val_loss += val_loss.item()\n",
        "            val_loop.set_description(f\"Validation Epoch [{epoch+1}/{num_epochs}]\")\n",
        "            val_loop.set_postfix(val_loss=running_val_loss / (i + 1))\n",
        "\n",
        "    val_losses.append(running_val_loss / len(val_loader))\n",
        "\n",
        "    # Update the learning rate using the scheduler\n",
        "    scheduler.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuOVwJrWPfpV"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_losses, label='ResNet Loss', linestyle='-', marker='o', color='blue')\n",
        "plt.plot(val_losses, label='ResNet Validation Loss', linestyle='-', marker='o', color='red')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrJiO9Tfz3Uv",
        "outputId": "3d98fe99-88b2-40a4-9d83-1d81a8ef448c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted_traits:\n",
            "emotion =  Anger\n",
            "Arousal =  0.62899756\n",
            "Valence =  0.06207083\n",
            "Real Emotion: Neutral\n",
            "Real Arousal: -0.200169\n",
            "Real Valence: 0.200169\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "# Usage example\n",
        "image_path = \"test_set/val_set/images/182.jpg\"\n",
        "landmarks_path = \"test_set/val_set/annotations/182_lnd.npy\"\n",
        "# model = model.to(device)\n",
        "arousal_valence, emotions = predict_image(image_path, landmarks_path, model, device, transform_train)\n",
        "emotion_dict = {0: \"Neutral\", 1: \"Happy\", 2: \"Sad\", 3: \"Surprise\", 4:\n",
        "\"Fear\", 5: \"Disgust\", 6: \"Anger\", 7: 'Contempt'}\n",
        "\n",
        "print(\"Predicted_traits:\")\n",
        "\n",
        "print(\"emotion = \", emotion_dict[emotions.item()])\n",
        "print(\"Arousal = \", arousal_valence[0][0])\n",
        "print(\"Valence = \", arousal_valence[0][1])\n",
        "\n",
        "print(\"Real Emotion:\", emotion_dict[int(np.load(\"test_set/val_set/annotations/184_exp.npy\").item())])\n",
        "print(\"Real Arousal:\", np.load(\"test_set/val_set/annotations/184_aro.npy\"))\n",
        "print(\"Real Valence:\", np.load(\"test_set/val_set/annotations/184_val.npy\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JL2PrrteK9hV"
      },
      "outputs": [],
      "source": [
        "model_path = \"densenet_back.pth\"\n",
        "torch.save(model.state_dict(), model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5fgSv_sKhoH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
